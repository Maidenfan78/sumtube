# SumTube â€“ YouTube Video Summariser

A oneâ€‘file CLI that turns any public YouTube URL into a tidy, ~120â€‘word summaryâ€”without needing a browser, clipboard juggling, or an expensive GPU. It grabs captions when available, falls back to audioÂ + Whisperâ€‘cpp when they donâ€™t, splits the transcript by GPT tokens, then fires the parts through the OpenAIÂ ChatÂ API in parallel.

Future releases will add an interactive â€œaskâ€‘theâ€‘videoâ€ box powered by retrievalâ€‘augmented generation (RAG) so users can query facts like *â€œWhich graphics card had the best priceâ€‘toâ€‘performance?â€* directly from long reviews.

---

## âœ¨ Features

* **Caption first, audio second** â€“ uses `youtube-transcript-api`; if no captions, downloads audio with `yt-dlp` and transcribes locally via `whispercpp` wheels (no compiler needed).  
* **Tokenâ€‘safe chunker** â€“ splits transcripts with `tiktoken`, never overflowing the 128â€¯k context window of GPTâ€‘4o.  
* **Fully async pipeline** â€“ leverages `openai.AsyncClient`, hitting OpenAI in parallel for 3â€‘4Ã— speedâ€‘ups on long videos.  
* **.env convenience** â€“ projectâ€‘root `.env` is autoâ€‘loaded by `pythonâ€‘dotenv`; no need to touch system variables.  
* **Rich terminal output** â€“ colourised summaries thanks to the `rich` library.  
* **MITâ€‘licensed & productionâ€‘ready** â€“ permissive license lets you embed the summariser anywhere.

---

## ğŸš€ QuickÂ Start

```bash
git clone https://github.com/<your-user>/sumtube.git
cd sumtube
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
echo "OPENAI_API_KEY=sk-..." > .env               # store your OpenAI key
python yt_summariser.py https://youtu.be/dQw4w9WgXcQ
```

Example output:

```
Summary: The video explains ... (â‰ˆ120 words)
```

> **Need an API key?** Create one at <https://platform.openai.com/account/api-keys> and paste it into `.env`.

---

## ğŸ› ï¸ Installation Details

| Dependency              | Why                           | Windows Wheels |
|-------------------------|-------------------------------|----------------|
| `youtube-transcript-api`| caption grabber               | âœ… |
| `yt-dlp`                | audioâ€‘only download           | âœ… |
| `whispercpp`            | local speechâ€‘toâ€‘text          | âœ… |
| `openai` â‰¥Â 1.14         | Chat & Whisper API            | n/a |
| `tiktoken`              | token counting                | âœ… |
| `python-dotenv`         | load `.env` file              | âœ… |
| `rich`                  | coloured CLI output           | âœ… |

Python 3.8â€¯â†’â€¯3.12 recommended; many AI wheels donâ€™t yet publish 3.13 binaries.

---

## ğŸ—ï¸ How It Works

1. **ID extraction** â€“ normalises both `youtu.be/<id>` and `watch?v=<id>` forms.  
2. **Transcription**  
   * **Captions:** `YouTubeTranscriptApi.get_transcript(video_id, languages=["en","en-US","auto"])`  
   * **Fallback STT:** `yt-dlp -x --audio-format m4a` â†’ `Whisper.from_pretrained("tiny.en").transcribe()`  
3. **Chunk & summarise**  
   * Encode transcript with `tiktoken`; slice every â‰¤â€¯3000 tokens.  
   * Asyncâ€‘call `gpt-4o` for each slice, then ask the model to fuse bulletâ€‘point results into one paragraph.  

All temporary audio files are autoâ€‘deleted after transcription.

---

## ğŸ“‹Â Roadmap

| Milestone                                   | Status |
|---------------------------------------------|--------|
| Interactive Q&A box with RAG (FAISS + LC)   | â³ |
| Web UI (FastAPI + HTMX)                     | â³ |
| Docker image & GitHub Action release        | â³ |
| GUI model selector & cost estimator         | â³ |

---

## ğŸ¤ Contributing

1. Fork the repo and create a feature branch.  
2. Write tests (`pytest`).  
3. Submit a PR.

Please run `black` and `ruff` before pushing and ensure no secrets are committed.

---

## ğŸ“ License

Released under the **MIT License** â€” see `LICENSE` for details.
